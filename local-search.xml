<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>基于PrimoCache的硬盘缓存加速</title>
    <link href="/2020/06/20/hd-cache/"/>
    <url>/2020/06/20/hd-cache/</url>
    
    <content type="html"><![CDATA[<p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/20200620173204.png"></p><p>目前随着硬盘发展的趋势，越来越多的硬盘采用了SMR存储技术，厂商依托这个技术节省了成本，但是用户却常常饱受硬盘掉速的困扰。</p><p><strong>如下图所示👇</strong></p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/1592645713715.jpeg">希捷酷鱼全线SMR</p><p>在进行大文件（10G左右）读写的时候就会有明显的感受，看着这十几万的延迟，不得不说👴要等吐了！</p><p>由此可见SMR硬盘是有多么依赖缓存空间！！！</p><h4 id="下面有请SMR救星PrimoCache上场！"><a href="#下面有请SMR救星PrimoCache上场！" class="headerlink" title="下面有请SMR救星PrimoCache上场！"></a>下面有请SMR救星<a href="https://www.romexsoftware.com/zh-cn/primo-cache/index.html">PrimoCache</a>上场！</h4><p>先看看加速效果：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/1592645718106.jpeg">过于暴力的测试数据</p><h3 id="PrimoCache的使用方法"><a href="#PrimoCache的使用方法" class="headerlink" title="PrimoCache的使用方法"></a>PrimoCache的使用方法</h3><ul><li><strong>前期准备</strong></li></ul><p>1）前往<a href="https://www.romexsoftware.com/zh-cn/primo-cache/index.html">PrimoCache官网</a>下载正版付费软件</p><p> 注：<a href="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/PrimoCache%20v3.09.zip">用于学习研究的小伙伴可以点这个链接</a>进行下载</p><p>2）按照流程进行安装（等一些列操作）</p><ul><li><strong>开始配置PrimoCache</strong></li></ul><p>1）创建加速任务</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/1.png"></p><p>2）选择需要加速的硬盘</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/2.png"></p><p>在这里选择你想要加速度的硬盘，通常选择你想要拯救的SMR盘</p><p>3）配置缓存策略</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/3.png"></p><p><strong>L1级缓存</strong></p><p>通过分配一部分内存空间来作为硬盘缓存分区，根据自己的电脑配置来分配空间大小。</p><p>通常来说不必分太大的内存，L1级缓存的作用一般只用于加速碎片文件的读写，体积通常不会太大。</p><p><strong>L2级缓存</strong></p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/primocache/4.png"></p><p>先将SSD分配一块空白区域，<strong>单独分为一块磁盘分区</strong>。</p><p>选择刚刚分好的分区，并分配用于读和写的缓存空间。</p><p><strong>缓存粒度</strong></p><p>数值越小 | 加速效果越好 | CPU占用越高</p><p>数值越大 | 加速效果次之 | CPU占用越低</p><p><strong>预取设置</strong></p><p>最好别开机预取，不然电脑开机得开几十分钟！！！</p><p><strong>注意：</strong>未开启预取，意味着如果在缓存没有完全写入目标磁盘前<strong>关机</strong>、<strong>断电</strong>等情况出现你将会面临<strong>数据丢失</strong>！！！</p><ul><li><strong>配置完毕点击启动！！！</strong></li></ul><h3 id="小技巧："><a href="#小技巧：" class="headerlink" title="小技巧："></a><strong>小技巧：</strong></h3><p>如何确定缓存是否完全写入目标磁盘?</p><ol><li>打开<strong>任务管理器</strong></li><li>点击<strong>性能</strong>查看磁盘详情</li><li>观察目标磁盘<strong>活动时间</strong>和<strong>延迟</strong>，如果活动时间占比长时间小于20%且延迟小于10ms基本可以判断已完成写入！</li></ol>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
      <category>IT小技巧</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cache</tag>
      
      <tag>smr</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BP神经网络</title>
    <link href="/2020/06/19/bpnn/"/>
    <url>/2020/06/19/bpnn/</url>
    
    <content type="html"><![CDATA[<p>人工神经网络(Artificial Neural Network,简称 ANN)是利用仿生原理对人脑神经网络的结构和功能的简化抽象。人工神经网络是一种可以并行分散处理数据，具有非线性映射、容错能力强、自适应学习进化的特点。BP人工神经网络是在此基础上，基于误差反向传播算法而设计的多层前向神经网络。因其能够实现输入与输出间任意的非线性映射，因此在模式识别、自适应控制、风险评估等领域有着广泛的应用。</p><p>BP(back propagation)神经网络是1986年由Rumelhart和McClelland为首的科学家提出的概念，是一种按照误差逆向传播算法训练的多层前馈神经网络，是应用最广泛的神经网络。</p><h4 id="BP神经网络的基本原理"><a href="#BP神经网络的基本原理" class="headerlink" title="BP神经网络的基本原理"></a>BP神经网络的基本原理</h4><p>BP算法在神经网络训练中是一个十分典型的学习算法。其构成如下：一个输入层，一个或者多个隐含层和一个输出层构成，各层又由若干个神经元节点构成，每个神经元的输出值由其输入值、传递函数、阈值所决定。神经网络的学习过程包括了正向传播和反向传播两个过程。正向传播的过程中，在输入层输出数据，经过隐含层一系列运算后，将输出层得到的结果与期望值比较，得到误差取值。若误差大于某个范围则进行反向传播，根据上一层的误差，对该层神经元的权值和阈值进行相应的修改，从而减小误差。如此循环，直到误差在精度范围内或者达到训练终止条件为止。</p><h4 id="BP神经网络的训练方法"><a href="#BP神经网络的训练方法" class="headerlink" title="BP神经网络的训练方法"></a>BP神经网络的训练方法</h4><p>BP神经网络的基本训练步骤如下：</p><ol><li>BP神经网络的构建和初始化，确定隐含层层数，确定各层的节点数，随机生成各个节点的权值和阈值；</li><li>输入训练数据集，训练输入数据以及其对应的输出数据；</li><li>根据输入样本计算输出数据；</li><li>根据输出数据和期望数据计算误差，并判定是否达到精度，若达到精度就停止运行并取得各节点权值和阈值，否则继续执行（5）；</li><li>根据误差反向传播，修正各层各节点的权值和阈值；</li><li>继续执行（3）。</li></ol><h4 id="基本BP神经网络的构建与训练（Python）"><a href="#基本BP神经网络的构建与训练（Python）" class="headerlink" title="基本BP神经网络的构建与训练（Python）"></a>基本BP神经网络的构建与训练（Python）</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> numpy as np<br><br><span class="hljs-comment">#sigmoid 激活函数</span><br><span class="hljs-attribute">def</span> sigmoid1(x):<br><span class="hljs-attribute">a</span>=<span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br><span class="hljs-attribute">return</span> a<br><br><span class="hljs-comment">#训练集</span><br><span class="hljs-attribute">training_set</span>=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]])<br><span class="hljs-attribute">yy</span>=np.zeros((<span class="hljs-number">4</span>,<span class="hljs-number">1</span>))<br><span class="hljs-attribute">eta</span>=<span class="hljs-number">0</span>.<span class="hljs-number">1</span><br><br><span class="hljs-comment">#定义连接权、阈值</span><br><span class="hljs-attribute">vih</span>=np.random.rand(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)#输入层与隐层连接权<br><span class="hljs-attribute">delt_vih</span>=np.zeros((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><span class="hljs-attribute">r</span>=np.random.rand(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)#隐层阈值<br><span class="hljs-attribute">delt_r</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br><span class="hljs-attribute">whj</span>=np.random.rand(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)#隐层与输出层连接权<br><span class="hljs-attribute">delt_whj</span>=np.zeros((<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><span class="hljs-attribute">o</span>=np.random.rand(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)#输出层阈值<br><span class="hljs-attribute">delt_o</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-comment">#创建隐层</span><br><br><span class="hljs-attribute">alph</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br><span class="hljs-attribute">b</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br><span class="hljs-attribute">e</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))#隐层梯度项<br><span class="hljs-comment">#创建输出层</span><br><span class="hljs-attribute">beita</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-attribute">y</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><span class="hljs-attribute">g</span>=np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))#输出层梯度项<br><span class="hljs-comment">#开始训练</span><br><span class="hljs-attribute">times</span>=input(How many times do you want to train:\n)<br><span class="hljs-attribute">times</span>=int(times)<br><span class="hljs-attribute">for</span> daishu in range(<span class="hljs-number">0</span>,times):<br><span class="hljs-attribute">for</span> train in range(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>):<br><span class="hljs-comment">#计算隐层的输入</span><br><span class="hljs-attribute">alph</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=vih[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*training_set[train,<span class="hljs-number">0</span>]+vih[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]*training_set[train,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">alph</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=vih[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]*training_set[train,<span class="hljs-number">0</span>]+vih[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]*training_set[train,<span class="hljs-number">1</span>]<br><span class="hljs-comment">#计算隐层输出</span><br><span class="hljs-attribute">b</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=sigmoid1(alph[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]-r[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-attribute">b</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=sigmoid1(alph[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]-r[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])<br><span class="hljs-comment">#计算输出层的输入</span><br><span class="hljs-attribute">beita</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=whj[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*b[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]+whj[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]*b[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-comment">#输出层输出</span><br><span class="hljs-attribute">y</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=sigmoid1(beita[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]-o[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-attribute">yy</span>[train,<span class="hljs-number">0</span>]=y[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-comment">#输出层梯度项</span><br><span class="hljs-attribute">g</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=y[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*(<span class="hljs-number">1</span>-y[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])*(training_set[train,<span class="hljs-number">2</span>]-y[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-comment">#隐层梯度项</span><br><span class="hljs-attribute">e</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=b[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*(<span class="hljs-number">1</span>-b[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])*(whj[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*g[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-attribute">e</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=b[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]*(<span class="hljs-number">1</span>-b[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])*(whj[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]*g[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-comment">#更新连接权和阈值</span><br><span class="hljs-attribute">delt_whj</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=eta*g[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*b[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">delt_whj</span>[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]=eta*g[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*b[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">whj</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=delt_whj[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]+whj[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">whj</span>[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]=delt_whj[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]+whj[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">delt_o</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=-(eta*g[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-attribute">o</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=delt_o[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]+o[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">delt_vih</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=eta*e[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*training_set[train,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">delt_vih</span>[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]=eta*e[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]*training_set[train,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">delt_vih</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=eta*e[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]*training_set[train,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">delt_vih</span>[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]=eta*e[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]*training_set[train,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">vih</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=vih[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]+delt_vih[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">vih</span>[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]=vih[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]+delt_vih[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">vih</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=vih[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]+delt_vih[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">vih</span>[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]=vih[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]+delt_vih[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">delt_r</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=-(eta*e[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br><span class="hljs-attribute">delt_r</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=-(eta*e[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])<br><span class="hljs-attribute">r</span>[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]=delt_r[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]+r[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">r</span>[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]=delt_r[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]+r[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-attribute">training_set</span>=training_set.astype(np.float64)<br><span class="hljs-attribute">training_set</span>[:,<span class="hljs-number">2</span>]=yy[:,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">print</span>(Output:)<br><span class="hljs-attribute">print</span>(training_set)<br></code></pre></td></tr></table></figure><p>测试结果如下：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/BPNN/BP.png" alt="/"></p><h4 id="BP神经网络函数逼近实验"><a href="#BP神经网络函数逼近实验" class="headerlink" title="BP神经网络函数逼近实验"></a>BP神经网络函数逼近实验</h4><p>网络构建模型：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/BPNN/%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%911-768x448.jpg">神经网络模型</p><p><strong>对函数 f(x)&#x3D;x2&#x2F;3 +0.9√(3.3-x2 )×sin(31.41593x)的拟合（Python ）</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> time<br><span class="hljs-comment">#计时</span><br>start=time.time()<br><span class="hljs-comment"># 取得样本</span><br>x = np.linspace(-<span class="hljs-number">1.81659</span>,<span class="hljs-number">1.81659</span>,<span class="hljs-number">100</span>)<span class="hljs-comment">#获取-10 到 10 之间距离相等的 10000 个点，作为 x 取值。</span><br>x_size = x.size<br>y = np.zeros((x_size,<span class="hljs-number">1</span>))<span class="hljs-comment">#生成 y 数组</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x_size):<br>y[i]= math.<span class="hljs-built_in">pow</span>(math.<span class="hljs-built_in">pow</span>(x[i],<span class="hljs-number">2</span>),<span class="hljs-number">1</span>/<span class="hljs-number">3</span>)+<span class="hljs-number">0.9</span>*math.<span class="hljs-built_in">pow</span>((<span class="hljs-number">3.3</span>-<br>math.<span class="hljs-built_in">pow</span>(x[i],<span class="hljs-number">2</span>)),<span class="hljs-number">0.5</span>)*math.sin(<span class="hljs-number">10</span>*math.pi*x[i])<span class="hljs-comment">#得到样本集</span><br><br><span class="hljs-comment">#网络参数</span><br>hidesize = <span class="hljs-number">5</span> <span class="hljs-comment">#隐层神经元个数</span><br>W1 = np.random.random((hidesize,<span class="hljs-number">1</span>)) <span class="hljs-comment">#输入层与隐层之间的权重</span><br>B1 = np.random.random((hidesize,<span class="hljs-number">1</span>)) <span class="hljs-comment">#隐含层神经元的阈值</span><br>W2 = np.random.random((<span class="hljs-number">1</span>,hidesize)) <span class="hljs-comment">#隐含层与输出层之间的权重</span><br>B2 = np.random.random((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)) <span class="hljs-comment">#输出层神经元的阈值</span><br>threshold = <span class="hljs-number">0.05</span><span class="hljs-comment">#速率</span><br>max_steps = <span class="hljs-number">1001</span><span class="hljs-comment">#训练次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x_</span>):<span class="hljs-comment">#激活函数</span><br>y_ = <span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+math.exp(-x_))<br><span class="hljs-keyword">return</span> y_<br><br><span class="hljs-comment">#开始训练</span><br>E = np.zeros((max_steps,<span class="hljs-number">1</span>))<span class="hljs-comment">#误差随迭代次数的变化</span><br>Y = np.zeros((x_size,<span class="hljs-number">1</span>)) <span class="hljs-comment"># 模型的输出结果</span><br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_steps):<br>temp = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x_size):<br><br><span class="hljs-comment">#正向传播</span><br>hide_in = np.dot(x[i],W1)-B1 <span class="hljs-comment"># 隐含层输入数据</span><br>hide_out = np.zeros((hidesize,<span class="hljs-number">1</span>)) <span class="hljs-comment">#隐含层的输出数据</span><br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(hidesize):<br>hide_out[j] = sigmoid(hide_in[j])<br>y_out = np.dot(W2,hide_out) - B2 <span class="hljs-comment">#模型输出</span><br>Y[i] = y_out<br>e = y_out - y[i] <span class="hljs-comment">#模型输出减去实际结果。得出误差</span><br><br><span class="hljs-comment">#反向传播</span><br>dB2 = -<span class="hljs-number">1</span>*threshold*e<span class="hljs-comment">#输出层阈值误差</span><br>dW2 = e*threshold*np.transpose(hide_out)<span class="hljs-comment">#输出层权值误差</span><br>dB1 = np.zeros((hidesize,<span class="hljs-number">1</span>))<span class="hljs-comment">#输入层阈值误差</span><br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(hidesize):<br>dB1[j] = np.dot(np.dot(W2[<span class="hljs-number">0</span>][j],sigmoid(hide_in[j])),(<span class="hljs-number">1</span>-sigmoid(hide_in[j]))*(-<br>*e*threshold)<br>dW1 = np.zeros((hidesize,<span class="hljs-number">1</span>))<span class="hljs-comment">#输出层权值误差</span><br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(hidesize):<br>dW1[j] = np.dot(np.dot(W2[<span class="hljs-number">0</span>][j],sigmoid(hide_in[j])),(<span class="hljs-number">1</span>-<br>sigmoid(hide_in[j]))*x[i]*e*threshold)<br><span class="hljs-comment">#参数修正</span><br>W1 = W1 - dW1<br>B1 = B1 - dB1<br>W2 = W2 - dW2<br>B2 = B2 - dB2<br>temp = temp + <span class="hljs-built_in">abs</span>(e)<br><br>E[k] = temp<br><span class="hljs-comment">#训练进度</span><br><span class="hljs-keyword">if</span> k%<span class="hljs-number">100</span>==<span class="hljs-number">0</span>:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;已训练%d 次&#x27;</span>%k)<br>stop=time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练用时%ds&#x27;</span>%(stop-start))<br><br><span class="hljs-comment">#训练结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输入层权值 w1：&#x27;</span>)<br><span class="hljs-built_in">print</span>(W1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输入层阈值 b1：&#x27;</span>)<br><span class="hljs-built_in">print</span>(B1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输出层权值 w2：&#x27;</span>)<br><span class="hljs-built_in">print</span>(W2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输出层阈值 b2：&#x27;</span>)<br><span class="hljs-built_in">print</span>(B2)<br><span class="hljs-comment">#拟合曲线</span><br>plt.figure()<br>plt.plot(x,y)<br>plt.plot(x,Y,color=<span class="hljs-string">&#x27;red&#x27;</span>,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.show()<br><span class="hljs-comment">#误差变化</span><br>steps = np.linspace(<span class="hljs-number">0</span>,max_steps,max_steps)<br>plt.figure()<br>plt.plot(steps,E)<br>plt.show()<br></code></pre></td></tr></table></figure><p><strong>第一组测试结果：</strong></p><p>样本数量 100，训练次数 1000，训练速率 0.05</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs inform7">已训练 0 次<br>训练用时 0s<br>已训练 100 次<br>训练用时 1s<br>已训练 200 次<br>训练用时 2s<br>已训练 300 次<br>训练用时 3s<br>已训练 400 次<br>训练用时 4s<br>已训练 500 次<br>训练用时 5s<br>已训练 600 次<br>训练用时 6s<br>已训练 700 次<br>训练用时 8s<br>已训练 800 次<br>训练用时 9s<br>已训练 900 次<br>训练用时 10s<br>已训练 1000 次<br>训练用时 11s<br>输入层权值 w1：<br><span class="hljs-comment">[<span class="hljs-comment">[ 3.12169479]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-0.12887541]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-3.3705294 ]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[ 0.68280681]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-0.15176392]</span>]</span><br>输入层阈值 b1：<br><span class="hljs-comment">[<span class="hljs-comment">[2.41933289]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[2.5423254 ]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[2.67626102]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[2.92251419]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[2.58144723]</span>]</span><br>输出层权值 w2：<br><span class="hljs-comment">[<span class="hljs-comment">[1.25280733]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[0.07273052]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[1.29053161]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[0.28913382]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[0.09012128]</span>]</span><br>输出层阈值 b2：<br><span class="hljs-comment">[<span class="hljs-comment">[-0.13334891]</span>]</span><br></code></pre></td></tr></table></figure><p>拟合曲线：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/BPNN/%E6%8B%9F%E5%90%88%E6%9B%B2%E7%BA%BF1.jpg" alt="拟合曲线-1"></p><p>误差曲线：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/BPNN/%E8%AF%AF%E5%B7%AE%E6%9B%B2%E7%BA%BF1.png" alt="误差曲线-1"></p><p><strong>第二组测试结果：</strong></p><p>样本数量 10000，训练次数 1000，训练速率 0.05</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs inform7">已训练 0 次<br>训练用时 1s<br>已训练 100 次<br>训练用时 114s<br>已训练 200 次<br>训练用时 227s<br><br>已训练 300 次<br>训练用时 341s<br>已训练 400 次<br>训练用时 454s<br>已训练 500 次<br>训练用时 567s<br>已训练 600 次<br>训练用时 680s<br>已训练 700 次<br>训练用时 793s<br>已训练 800 次<br>训练用时 907s<br>已训练 900 次<br>训练用时 1020s<br>已训练 1000 次<br>训练用时 1134s<br>输入层权值 w1：<br><span class="hljs-comment">[<span class="hljs-comment">[-9.34109363]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-6.26655843]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[ 0.38915887]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-2.666515 ]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-2.69118967]</span>]</span><br>输入层阈值 b1：<br><span class="hljs-comment">[<span class="hljs-comment">[ 1.47712007]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[ 4.86462404]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-2.89037925]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[ 1.12851018]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[ 1.31828715]</span>]</span><br>输出层权值 w2：<br><span class="hljs-comment">[<span class="hljs-comment">[3.28226408]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[4.90921857]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[4.96920982]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-4.36115203]</span></span><br><span class="hljs-comment"><span class="hljs-comment">[-4.59367305]</span>]</span><br>输出层阈值 b2：<br><span class="hljs-comment">[<span class="hljs-comment">[3.29980763]</span>]</span><br></code></pre></td></tr></table></figure><p>拟合曲线：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/BPNN/%E6%8B%9F%E5%90%88%E6%9B%B2%E7%BA%BF2.jpg" alt="拟合曲线-2"></p><p>误差曲线：</p><p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/BPNN/%E8%AF%AF%E5%B7%AE%E6%9B%B2%E7%BA%BF2.png" alt="误差曲线-2"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Algorithm</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ai</tag>
      
      <tag>nn</tag>
      
      <tag>pnn</tag>
      
      <tag>algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>感知器（神经网络模型）</title>
    <link href="/2020/06/19/pnn/"/>
    <url>/2020/06/19/pnn/</url>
    
    <content type="html"><![CDATA[<p><strong>感知器</strong>是人工神经网络中的一种典型结构， 它的主要的特点是结构简单，对所能解决的问题 存在着收敛算法，并能从数学上严格证明，从而对神经网络研究起了重要的推动作用。</p><p><strong>感知器</strong>，也可翻译为<strong>感知机</strong>，是Frank Rosenblatt在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈式人工神经网络，是一种二元线性分类器。 Frank Rosenblatt给出了相应的感知器学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知器模型。</p><p>感知器是生物神经细胞的简单抽象，如右图.神经细胞结构大致可分为：树突、突触、</p><p><a href="https://baike.baidu.com/pic/%E6%84%9F%E7%9F%A5%E5%99%A8/16525448/0/810a19d8bc3eb1357c1bdf11a41ea8d3fc1f44bf?fr=lemma&ct=single" title="图1.神经细胞示意图"><img src="https://bkimg.cdn.bcebos.com/pic/810a19d8bc3eb1357c1bdf11a41ea8d3fc1f44bf?x-bce-process=image/resize,m_lfit,w_220,h_220,limit_1" alt="图1.神经细胞示意图"></a></p><p>神经细胞示意图</p><p>细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激动时为‘是’，而未激动时为‘否’。</p><p> 神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。 在人工神经网络领域中，感知器也被指为单层的人工神经网络，以区别于较复杂的多层感知器（Multilayer Perceptron）。 作为一种线性分类器，（单层）感知器可说是最简单的前向人工神经网络形式。尽管结构简单，感知器能够学习并解决相当复杂的问题。感知器主要的本质缺陷是它不能处理线性不可分问题。</p><h3 id="模型构建的代码实现（Python）"><a href="#模型构建的代码实现（Python）" class="headerlink" title="模型构建的代码实现（Python）"></a>模型构建的代码实现（Python）</h3><p>单层感知器模型</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import numpy as np<br><span class="hljs-comment">#sigmoid 激活函数</span><br>def nonlin(x, deriv = <span class="hljs-literal">False</span>):<br><span class="hljs-keyword">if</span>(<span class="hljs-attribute">deriv</span>==True):<br>return x*(1-x)<br>return 1/(1+np.exp(-x))<br><br><span class="hljs-comment">#输入-输出</span><br><span class="hljs-comment">#[0,0,1]-0</span><br><span class="hljs-comment">#[0,1,1]-1</span><br><span class="hljs-comment">#[1,0,1]-1</span><br><span class="hljs-comment">#[1,1,1]-0</span><br><br><span class="hljs-comment">#输入矩阵</span><br><span class="hljs-attribute">X</span>=np.array([[0,0,1],<br>[0,1,1],<br>[1,0,1],<br>[1,1,1]])<br><br><span class="hljs-comment"># 目标矩阵</span><br><span class="hljs-attribute">y</span>=np.array([[0,1,0,1]]).T<br><br><span class="hljs-comment">#生成随机权重</span><br>np.random.seed()<br>weight = np.random.random((3,1))<br><br><span class="hljs-comment">#训练次数</span><br><span class="hljs-attribute">times</span>=input(How many times you want <span class="hljs-keyword">to</span> train:\n)<br><span class="hljs-attribute">times</span>=int(times)<br><br><span class="hljs-comment">#训练开始</span><br><span class="hljs-keyword">for</span> iter <span class="hljs-keyword">in</span> range(times):<br>#计算输出<br><span class="hljs-attribute">L0</span>=X<br><span class="hljs-attribute">L1</span>=nonlin(np.dot(L0,weight))<br>#计算误差<br><span class="hljs-attribute">L1_error</span>=y-L1<br>L1_delta = L1_error*nonlin(L1,<span class="hljs-literal">True</span>)<br>#更新权重<br><br>weight+=np.dot(L0.T,L1_delta)<br><br><span class="hljs-built_in">print</span>(Output After Training:)<br><span class="hljs-built_in">print</span>(L1)<br><span class="hljs-built_in">print</span>(The Weight After Training:)<br><span class="hljs-built_in">print</span>(weight)<br><br></code></pre></td></tr></table></figure><p>测试结果：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs lua">How many times you want to train:<br><span class="hljs-number">10000</span><br>Output After Training:<br><span class="hljs-string">[[0.00966633]</span><br><span class="hljs-string">[0.99359071]</span><br><span class="hljs-string">[0.00786352]</span><br><span class="hljs-string">[0.99211866]]</span><br>The Weight After Training:<br><span class="hljs-string">[[-0.20823225]</span><br><span class="hljs-string">. [ 9.67307298]</span><br><span class="hljs-string">. [-4.6294448 ]]</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Algorithm</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ai</tag>
      
      <tag>nn</tag>
      
      <tag>pnn</tag>
      
      <tag>algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>linux 命令行下配置 wlan 无线网卡</title>
    <link href="/2020/06/18/linux-wifi/"/>
    <url>/2020/06/18/linux-wifi/</url>
    
    <content type="html"><![CDATA[<p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Gallery/2022/11/28/1024px-WiFi_Logo.svg.png"><br>Linux 命令行下配置连接 wlan 无线网卡具体步骤参考。</p><p>工作的大体思路如下：</p><ul><li>用iwconfig开启无线网卡的电源，并查找区域内的无线网络</li><li>连接到相应的无线网络</li><li>通过ifconfig启用无线网卡，并获取IP（如果使用DHCP的话）</li></ul><p><strong>注意：</strong></p><p>假设无线被识别为 wlan0，如果您的网卡没有被识别为 wlan0，可以在操作时做相应的修改。</p><p><strong>具体步骤</strong>：</p><ul><li>打开无线网卡电源</li></ul><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">iwconfig</span> wlan0 txpower <span class="hljs-literal">on</span><br></code></pre></td></tr></table></figure><ul><li>列出区域内的无线网络</li></ul><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">iwlist wlan0 <span class="hljs-built_in">scan</span><br></code></pre></td></tr></table></figure><ul><li>假设要连接到网络MyHome（即essid为MyHome的网络），那么输入命令</li></ul><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">iwconfig</span> wlan0 essid “MyHome”<br></code></pre></td></tr></table></figure><ul><li>如果网络是加密的，密码是0123456789，那么就输入命令</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">iwconfig</span> wlan0 essid “MyHome” key <span class="hljs-number">0123</span>-<span class="hljs-number">4567</span>-<span class="hljs-number">89</span><br></code></pre></td></tr></table></figure><ul><li>如果正常的话，输入</li></ul><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">iwconfig</span> wlan0<br></code></pre></td></tr></table></figure><p>就可以看到连接正常的各项参数了。</p><ul><li>启用无线网卡</li></ul><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">ifconfig</span> wlan0 up<br></code></pre></td></tr></table></figure><ul><li>如果是用 DHCP 获取 IP 的，那么用 dhclient 或 dhcpcd 获取 ip</li></ul><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">dhclient</span> wlan0<br></code></pre></td></tr></table></figure><p>或</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">dhcpcd wla<span class="hljs-symbol">n0</span><br></code></pre></td></tr></table></figure><ul><li>现在无线网卡应该可以正常使用了</li></ul><p><strong>注意：一定要把 NetworkManager 服务停掉</strong></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
      <tag>wifi</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ras-Pi的VPN设置</title>
    <link href="/2020/06/18/raspi-vpn/"/>
    <url>/2020/06/18/raspi-vpn/</url>
    
    <content type="html"><![CDATA[<p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/rasp-pi_vpn/86d8d8ae30cd6d7043056c6766004a68_waifu2x_4x_2n_jpg-768x699.png"></p><h2 id="基于PPTP协议的VPN连接"><a href="#基于PPTP协议的VPN连接" class="headerlink" title="基于PPTP协议的VPN连接"></a>基于PPTP协议的VPN连接</h2><h3 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h3><ol><li>拥有一台已经配置好的PPTP服务器</li><li>可以正常使用的任意Linux发行版（Debian为例）</li></ol><h3 id="操作方法："><a href="#操作方法：" class="headerlink" title="操作方法："></a>操作方法：</h3><ul><li>安装PPTP-Linux客户端</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sudo apt-<span class="hljs-built_in">get</span> install -y pptp-linux<br></code></pre></td></tr></table></figure><ul><li>配置客户端信息</li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs awk">sudo vim <span class="hljs-regexp">/etc/</span>ppp<span class="hljs-regexp">/peers/</span>pptpconf<br><br><span class="hljs-regexp">//</span> 添加如下内容：（自行更改IP, name, password）<br>pty pptp xxx.xxx.xxx.xxx --nolaunchpppd<br>name xxx<br>password xxx<br>remotename PPTP<br>require-mppe-<span class="hljs-number">128</span><br>require-mschap-v2<br>refuse-eap<br>refuse-pap<br>refuse-chap<br>refuse-mschap<br>noauth<br>persist<br>maxfail <span class="hljs-number">0</span><br>defaultroute<br>replacedefaultroute<br>usepeerdns<br></code></pre></td></tr></table></figure><ul><li>启动和关闭</li></ul><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">pptp</span> 客户端操作： <br>启动：pon pptpconf <br>关闭：poff pptpconf<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>raspberry-pi</tag>
      
      <tag>vpn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用docker部署WordPress</title>
    <link href="/2020/06/18/wordpress-docker/"/>
    <url>/2020/06/18/wordpress-docker/</url>
    
    <content type="html"><![CDATA[<p><img src="https://wpcos-1300629776.cos.ap-chengdu.myqcloud.com/Picture/wp/article/202006/wordpress_docker/34173314725397cecc026b7371fa78ee-1024x640.jpg"></p><h2 id="基于Docker的部署方案"><a href="#基于Docker的部署方案" class="headerlink" title="基于Docker的部署方案"></a>基于Docker的部署方案</h2><p>容器部署在Docker下部署WordPress的难点在于<strong>数据库的连接</strong>，下面给出一种可行的方案。</p><h3 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h3><ul><li>任意可以运行的Linux发行版</li><li>已安装完成Docker相关组件</li><li>完成了Docker镜像源相关的设置</li></ul><h3 id="安装步骤："><a href="#安装步骤：" class="headerlink" title="安装步骤："></a>安装步骤：</h3><h4 id="1-数据库容器配置"><a href="#1-数据库容器配置" class="headerlink" title="1. 数据库容器配置"></a>1. 数据库容器配置</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">docker pull MariaDB <span class="hljs-regexp">//</span>下载mysql镜像<br>docker volume create mysql1 <span class="hljs-regexp">//</span>创建数据卷<br>docker run -d --privileged=true --name my-mysql -v <span class="hljs-regexp">/data/my</span>sql:<span class="hljs-regexp">/var/</span>lib/mysql -e MYSQL_ROOT_PASSWORD=<span class="hljs-number">123456</span> -p <span class="hljs-number">33306</span>:<span class="hljs-number">3306</span> mariadb<br></code></pre></td></tr></table></figure><p>参数解释:</p><ul><li>-p: 端口映射，33306表示宿主，3306表示容器中的端口。 这里表示将宿主机的33306映射给镜像的3306。</li><li>-e: 环境变量， 环境变量和具体的Docker容器制作时设置有关，这里表示设置镜像中MySQL的root 密码时123456</li><li>-v: 指定数据卷，也就是将我们MySQL容器的&#x2F;var&#x2F;lib&#x2F;mysql映射到宿主机的&#x2F;data&#x2F;mysql</li><li>–privileged&#x3D;true: CentOS系统下的安全Selinux禁止了一些安全权限，导致MySQL容器在运行时会因为权限不足而报错，所以需要增加该选项</li></ul><h4 id="2-WordPress容器配置"><a href="#2-WordPress容器配置" class="headerlink" title="2. WordPress容器配置"></a>2. WordPress容器配置</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">docker pull wordpress <span class="hljs-comment">//下载镜像</span><br>docker run -d <span class="hljs-attr">--name</span> my-wp -e WORDPRESS_DB_HOST=mysql -e WORDPRESS_DB_PASSWORD=<span class="hljs-number">123456</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-number">8080</span>:<span class="hljs-number">80</span> <span class="hljs-attr">--link</span> my-mysql:mysql wordpress <span class="hljs-comment">//运行镜像创建容器</span><br></code></pre></td></tr></table></figure><p>参数解释：</p><ul><li>“WORDPRESS_DB_HOST”: 链接的docker的MySQL的IP地址和端口，一般设置成mysql表示用默认的设置</li><li>“WORDPRESS_DB_USER”: 以什么用户使用MySQL，默认是root</li><li>“WORDPRESS_DB_PASSWORD” 这设置MySQL的登陆用户密码，由于上一项是默认的root，所以这一项和之前的MYSQL_ROOT_PASSWORD“要相同。</li><li>WORDPRESS_DB_NAME”: 数据库的表名，不需要修改，用默认的”wordpress就行</li></ul><h4 id="3-进入Web界面设置"><a href="#3-进入Web界面设置" class="headerlink" title="3. 进入Web界面设置"></a>3. 进入Web界面设置</h4><p>浏览器地址栏输入IP:Port访问即可</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Web</category>
      
    </categories>
    
    
    <tags>
      
      <tag>docker</tag>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  
  
  
  <entry>
    <title>about</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[]]></content>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[@font-face {  font-family: "iconfont"; /* Project id 3795310 */  src: url('//at.alicdn.com/t/c/font_3795310_jg4xnoqq9a.woff2?t=1669558765053') format('woff2'),       url('//at.alicdn.com/t/c/font_3795310_jg4xnoqq9a.woff?t=1669558765053') format('woff'),       url('//at.alicdn.com/t/c/font_3795310_jg4xnoqq9a.ttf?t=1669558765053') format('truetype');}.iconfont {  font-family: "iconfont" !important;  font-size: 16px;  font-style: normal;  -webkit-font-smoothing: antialiased;  -moz-osx-font-smoothing: grayscale;}.icon-guanli:before {  content: "\e600";}.icon-fenlei:before {  content: "\e71b";}]]></content>
    
  </entry>
  
  
  
</search>
